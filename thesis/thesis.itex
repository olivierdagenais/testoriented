<?xml version="1.0" encoding="UTF-8"?>
<iTeX>
  <preamble><!-- !TEX TS-program = pdflatex -->
    <!-- !TEX encoding = UTF-8 Unicode -->

    <command name="documentclass" opt="12pt" param="dalthesis" />

    <command name="usepackage" opt="utf8" param="inputenc" /> <!-- set input encoding (not needed with XeLaTeX) -->
    
    <!-- disable turning "fi", "ff", etc. into one character: http://www.latex-community.org/forum/viewtopic.php?f=5&t=953  -->
    <command name="usepackage" param="microtype" />
    <command name="DisableLigatures" param="encoding = *, family = *" />
  </preamble>
  <document>
    <group name="Title page">
      <command name="title" param="Stateless Programming And Testing: \\ Extracting a functional layer from object-oriented code" />
      <command name="author" param="Olivier Dagenais" />
      <command name="submitdate" param="July 29, 2011" />
      <command name="copyrightyear" param="2011" />
      <command name="degree" param="Master of Computer Science" />
    </group>

    <command name="frontmatter">
      <block param="abstract">
        <!-- An abstract is required in all theses. -->
        <!-- Make sure that it fits on one page! -->
      </block>
      <block param="acknowledgements">
        <!-- This is the acknowledgements. -->
        <!-- It is optional. -->
      </block>
    </command>
    <command name="mainmatter">
      <command name="chapter" param="Introduction">
        <command name="section" param="Introduction">
          This chapter introduces the problem and the motivation for solving it, as well as the overall goals and specific objectives covered by the present thesis.  It concludes with an overview of the rest of the document.
        </command>
        <command name="section" param="Problem">
          Software has advanced considerably in the last "many" years, but has software quality kept up?  We don't always know how good a program or component is or if it is free of defects, if only those defects that will affect us, directly or otherwise.

          Programming languages and compilers evolved to add features that make it easier to write code, although those features don't always make it easier to test the same code.  Object-oriented programming, for example, has introduced constructs and paradigms designed to make abstraction, inheritance and polymorphism easier, but at the same time introduced a feature that can make testing more difficult:  encapsulation (i.e. information hiding) in the form of instance state.  Alan Kay wrote that object state was meant to be immutable <!-- TODO: reference --> but that intent is seldom enforced by programming languages, frameworks or toolkits.

          Hidden and mutable instance state poses a problem for unit testing because that instance state may come into play when exercising a specific scenario.  Because it is mutable, the values may not be initialized at instance construction or can change as instance methods are called.  Because it is hidden, there exists, by definition, no way to manipulate this instance state directly.  

          <command name="subsection" param="Hidden mutable state, described">
            Hidden mutable state (HMS) manifests itself in a few different forms:
            <block param="itemize">
              <command name="item"> Instance state that cannot be set from the constructor or a factory method.</command>
              <command name="item"> Instance state that cannot be directly set from a setter method or property.</command>
              <command name="item"> A constructor or direct factory method is not available.  In other words, the number of steps to create an instance of the class under test is greater than 1.</command>
              <command name="item"> Instance state points to objects with their own hidden mutable state.</command>
            </block>
            In other words, a class has instance state that must be modified indirectly as a result of calling a method that, although it may not be directly related, nonetheless modifies one or more fields.
          </command>

          The result of hidden mutable state is we end up with lots of difficult-to-test code, which means it stays untested, even despite the many automated test frameworks and systems <!-- TODO: reference http://opensourcetesting.org/ -->.  This is likely due to a tradeoff between ease of maintenance <!-- TODO: modification? --> and testability <!-- TODO: find source --> where testability was ultimately not favoured.

          We still want to test the code, but we must first surmount the difficult-to-test obstacle that hidden mutable state introduces.  In the process of testing a method that uses hidden instance state (as input, output or both), it usually becomes necessary to write the unit test such that it will indirectly orchestrate said instance state so that specific scenarios can be implemented <!-- TODO: instrumented/probed/explored? -->.  This makes the test method difficult to maintain because the "arrange" phase will be more unwieldy and less obvious, which will have the effect of discouraging the creation of such tests.
            <!-- TODO:
            = fields of abstract type (i.e. System.IO.Stream)
            = environment-related values (input domain is very constrained and specialized)
            -->
        </command>
        <command name="section" param="Motivation">
          <!-- Explain why some people would care -->
          <!-- TODO: 
          = should I motivate [unit] testing more, like I did in my paper?
          = how about talking about the applicability to code that is currently without tests or with insufficient tests
          -->
          Lest we could somehow perform the practically impossible "complete testing", <!-- TODO: cite a source? --> testing can only prove the presence of defects and not their absence <!-- TODO: cite Dijkstra -->.  A good set of tests, on the other hand, can catch software regressions that may be introduced through the addition of a feature or the removal of a defect.  Finding regressions as early is possible ensures their fixing cost is minimized <!-- TOOD: source, probably Joel Spolsky -->.  A good set of tests is henceforth defined as one that can catch such software regressions.

          A metric commonly used to represent confidence in a set of tests is code coverage percentage <!-- TODO: source -->, usually in the form of branch coverage <!-- TODO: source -->.  Code that is covered might be - but is not necessarily - tested, but uncovered code is definitely untested code.  Shipping software with untested code is like serving a dish that has not yet been tasted.

          Hidden mutable state (HMS) does not just present trouble to human-generated tests (HGT), but also to computer-generated tests (CGT), such as those generated through Evolutionary Testing (ET), a form of Search Based Software Testing (SBST).  <!-- TODO: does Concolic Testing also suffer from the state problem? -->  Indeed, McMinn reported on the difficulty HMS brought to ET <command name="cite" param="Mcminn03thestate" />:
          <block param="quote">States can cause a variety of problems for ET, since test 
goals involving states can be dependent on the entire history of input to the test object, as well as just the current input.
          </block>

          Although it is possible to work around some HMS with a noteworthy "arrange" phase, this is not always desirable as it can increase the maintenance overhead and costs to write the unit tests, both for human- and computer-generated tests.  There are other options available, such as increasing the visiblity of the mutable state or converting the state from mutable to read-only, but both options would likely change the public interface of the class under test to an unacceptable level, not to mention the associated risk in performing non-trivial modifications without the safety net of sufficient tests.
        </command>
        <command name="section" param="Goals">
          <!-- Can address only part of the problem
          Something abstract/general, such as "world hunger", or a subset of it "hunger in Ottawa"
          Can be written retroactively, after the results have been written -->
          If we are to defeat the difficulties of hidden mutable state (HMS), can we do so with minimal changes to the public interface and with minimal risk of introducing regressions?  Can the unit tests be kept parsimonious, yet cover all the important edge cases?  Is there something we can do to make code containing hidden mutable state more easily testable?

          The goal of this thesis (paper?) is to introduce and motivate the use of a testability refactoring <!-- TODO: cite "Refactoring as testability transformation" --> that extracts a functional (as in state-free) software layer to specifically work around many of the <!-- [testability] --> problems of HMS and to generally increase the testability of the code under test, while minimally affecting maintainability and the public interface of the code under test.  The goal is to be validated according to the following metrics:
          
          <block param="enumerate">
            <command name="item"> Complexity of the unit tests.
            <!-- TODO: 
            = i.e. Make unit tests as small and simple as possible.
            == Count not just "arrange" but also "clean-up"
            == code that computes tax and discount at the same time makes it harder to test tax and discount computations separately
            = complexity can be measured as:
            == cyclomatic complexity
            == number of instructions
            == number of methods executed?
            == number of lines of code
            -->
            </command>
            <command name="item"> Number of changes to class under test's public interface.</command>
            <command name="item"> Percentage of branches covered using concolic testing <!-- TODO: source -->.
            <!-- TODO:
            = i.e. Test as much code as feasible
            = when I was testing with Pex, I think the fact that methods were introduced was skewing the percentage numbers; can we account for that?
            = ultimately, the previous metric is a measure of reachability.  Is there a better one?
            -->
            <!-- TODO: can maintainability be measured? -->
            <!-- TODO:
            = can I measure the risk of introducing defects or regressions?
            = in other words, what is the safety of the testability transformation/refactoring?
            -->
            </command>
          </block>
          <!--
          = even though I mention ET in the motivation section, I have no goal to evaluate anything for ET???
          -->
        </command>
        <command name="section" param="Objectives">
          <!-- (what you will produce to meet your goal)
          action items to address goal -->
          To meet the goal, the some experiments/evaluations will be performed using the source code from a specially-crafted project (designed to expose problematic scenarios) as well as the following open-source projects:
          <block param="enumerate">
            <command name="item"> AtomicCms</command>
            <command name="item"> KeePassLib</command>
            <command name="item"> PivotStack</command>
            <command name="item"> Textile</command>
          </block>

          Unit tests will be written for select functionality from all projects using the following approaches/strategies:
          <block param="enumerate">
            <command name="item"> No modifications to the code under test</command>
            <command name="item"> Stateless method extraction</command>
            <command name="item"> Visibility increases</command>
            <command name="item"> Mutation removal</command>
            <command name="item"> Private accessors</command>
            <command name="item"> Test helper methods</command>
          </block>
          
          <!-- TODO: is it necessary to include this next part? -->
          The following evaluations will be made against the results of writing tests using each of the 6 strategies on each of the 5 projects:
          <block param="enumerate">
            <command name="item"> Complexity of the unit tests.</command>
            <command name="item"> Number of changes to class under test's public interface.</command>
            <command name="item"> Percentage of branches covered using concolic testing.</command>
          </block>
          
          <command name="subsection" param="Implementation details">
            <!-- TODO: isn't this the kind of stuff we would find in the Approach chapter? -->
            All source code is in C#, targetting the .NET runtime.  Unit tests will be written or generated using the NUnit framework.  The concolic testing tool used will be Pex.
          </command>
        </command>
        <command name="section" param="Outline">
          ...
        </command>
        <command name="section" param="Summary">
          <!-- key points of what was presented and linkage to next chapter -->
          ...
        </command>
      </command>
      <command name="chapter" param="Background">
        <command name="section" param="Introduction">
          <!-- what the chapter is about -->
          ...
        </command>
        <command name="section" param="Object oriented programming">
          <!-- TODO:
          = classes
          = instance fields, methods, properties
          = static fields, methods, properties
          = interfaces
          -->
        </command>
        <command name="section" param="Automated testing">
          <!-- TODO:
          = intro
          = I think it was Binder who painfully explained these test frameworks from the point of view of someone who has only ever done manual testing
          -->
          <command name="subsection" param="Testing frameworks">
            <!-- TODO:
            = xUnit
            = arrange, act, assert
            = integration with a build process to further establish confidence
            -->
          </command>
          <command name="subsection" param="Test scope">
            <!-- TODO:
            = unit tests vs. integration tests vs. ???
            -->
          </command>
          <command name="subsection" param="Code coverage">
          </command>
        </command>
        <command name="section" param="Motivations for Computer-Generated Tests">
          <!-- TODO:
          = see notes about the reasons given by the car guys (computer-generated code, integration/co-operation testing of components written by many parties, expensive manual testing like driving the car, etc.)
          -->
        </command>
        <command name="section" param="Intents of Computer-Generated Tests">
          There are four major reasons <!-- TODO: is it a reason? how about objective? --> to use Computer-Generated Tests (CGT).
          <command name="subsection" param="Structural Testing">
          </command>
          <command name="subsection" param="Functional Testing">
          </command>
          <command name="subsection" param="Non-functional Testing">
          </command>
          <!-- <command name="subsection" param="TODO: 4th Intent of CGT">
          </command> -->
        </command>
        <command name="section" param="Approaches to Computer-Generated Tests">
          There are three major approaches in generating tests. <!-- TODO: cite "An empirical investigation into branch coverage for C programs using CUTE and AUSTIN" -->
          <command name="subsection" param="Random Testing">
          </command>
          <command name="subsection" param="Search Based Software Testing">
            <!-- 
            = Also called metaheuristic
            = Hill climbing, simulated annealing
            = Evolutionary Testing
            -->
          </command>
          <command name="subsection" param="Concolic Testing">
            <!--
            = Concrete + Symbolic = Concolic (a.k.a. dynamic symbolic execution)
            = Constraint solvers
            = Pex
            -->
          </command>
        </command>
        <command name="section" param="Related work">
          <!--
          = Miško Hevery
          == testability explorer
          === Non-Mockable Total Recursive Cyclomatic Complexity
          === Global Mutable State
          === Law of Demeter
          == "How to think about OO"
          == "Static Methods are Death to Testability"
          = FxCop/Code Analysis tools
          == "CA1502: Avoid excessive complexity" http://msdn.microsoft.com/en-us/library/ms182212.aspx
          === "A low cyclomatic complexity generally indicates a method that is easy to understand, test, and maintain."
          = The same data dependency analysis used in Mcminn03thestate can also be used to determine the arguments to a stateless method
          = any SBST research that aims to work around state problems
          == GP then GA
          == Data Dependency Analysis
          == temporary program transformations
          -->
          ...
        </command>
        <command name="section" param="Context">
          <!-- Show that I understand what everybody else has done ("master an area") -->
          ...
          Binder, page 52 introduces a few relevant concepts
        </command>
        <command name="section" param="Scope">
          <!-- End with feature matrix, comparing my solution to others -->
          <!--
          = contrast with isolated, stand-alone tests
          = contrast with test helper methods
          == how about test helper properties?
          == For example, DeepZoomImageTest.TestComputeLevelSize() that creates two instances per call:
          Assert.AreEqual (new Size (1200, 1500), TestComputeLevelSize (PortraitImageSize, 12));
          vs. the original stateless method that could be called directly:
          Assert.AreEqual (new Size (1200, 1500), DeepZoomImage.ComputeLevelSize (PortraitImageSize, 12));
          === For that particular one, because my settings class performs work in the constructor, there are some properties which MUST be provided.
          == they shift complexity instead of removing it altogether.  I need to find some way to prove that this leads to more brittle tests.  Perhaps the fact that the stateless method is reusable vs. the test helper method isn't?
          = contrast with better abstractions/use of interfaces
          == virtual/in-memory file system
          == operating on sequences instead of streams
          == introducing a class for private methods that coordinate related pieces of data
          == introducing a functor for methods that coordinate related pieces of data
          == introducing a generic class to remove copy/pasted typed implementations (i.e. FooCollection, BarCollection replaced by Collection{Foo} and Collection{Bar})
          == a regular expression toolkit or compiler that generates a model specific to the specified expression
          = contrast with visible, mutable state
          == changes current public interface
          = contrast with hidden, immutable state
          == changes current public interface
          = contrast with no state whatsoever
          == changes current public interface
          = contrast with static code analysis
          == contrast with code contracts
          = contrast with code reviews
          = contrast with dependency injection
          = contrast with improving the tools (i.e. research in SBST and DSE)
          = contrast with temporary program transformations (i.e. non-refactoring)
          
          Not in scope:
          = parallelization of tests (although probably easier with technique)
          = difficult-to-test object state changes?
          -->
          ...
        </command>
        <command name="section" param="Summary">
          <!-- key points of what was presented and linkage to next chapter -->
          ...
        </command>
      </command>
      <command name="chapter" param="Approach">
        <command name="section" param="Introduction">
          <!-- what the chapter is about -->
          ...
        </command>
        <command name="section" param="Design">
          <!-- 
          = discuss applicability/suitability (in 1 or 2 sections?)
          == existing/legacy systems with few tests (through acquisition, maintenance)
          == technology constraints (i.e. class must have default public constructor, such as IHttpModule implementations in ASP.NET)
          == programming language with visibility controls for methods such that the extracted methods are visible to tests but not necessarily all users
          = discuss theoretical trade-offs and suitability towards goal
          == forces number of arguments of stateless method to remain small, to avoid Misko's splippery slope
          == MUST be a pure refactor, i.e. callers are/will be none the wiser
          === no bugs added
          === no bugs fixed during this step, always expose and fix bug separately from extraction
          === no performance differences (extracted method could always be inlined by the compiler for release)
          == MUST preserve seams, in case code is already partially tested
          => work around reachability barriers
          = discuss scope of technique (i.e. on which code should it be applied)
          == what to extract, what not to extract
          == what can be re-ordered because its exact "timing" is not terribly important
          == how many [static/instance] fields read
          == how many [static/instance] fields modified
          == how many parameters needed
          == what happens when environment-modifying operations are used
          = discuss experiments with Pex (if only to introduce for another section)
          == trade-offs of automatic test generation based on code coverage
          == differences between a constraint solver and evolutionary testing
          = manual application
          == manual test writing
          == manual statelesss method extraction
          == compare unit tests before and after
          -->
          <!-- 
          = reduce the number of inputs participating in the test
          == reduce the number of methods called to bring object under test into appropriate state (methods executed before - arrange)
          == reduce the number of instances created to support test scenario
          = reduce the number of outputs produced by the test
          == reduce the number of methods called to extract interesting output (methods executed after - assert)
          = reduce the amount of clean-up done to compensate for "arrange" phase
          -->
          ...
        </command>
        <command name="section" param="Decisions made">
          <!--
          = {Project}.ManualTests created to avoid interference with Pex if had used {Project}.Tests
          -->
          ...
        </command>
        <command name="section" param="Reproducability">
          <!-- Enough for somebody to reproduce experiment -->
          ...
        </command>
        <command name="section" param="Summary">
          <!-- key points of what was presented and linkage to next chapter -->
          ...
        </command>
      </command>
      <command name="chapter" param="Results and Validation">
        <command name="section" param="Introduction">
          <!-- what the chapter is about -->
          ...
        </command>
        <command name="section" param="Results">
          ...
        </command>
        <command name="section" param="Detailed results">
          <!--
          = applying the stateless method extraction technique to the open-source projects
          = applying the technique to examples provided in original paper
          = applying the technique backwards to PivotStack
          = evaluating with code coverage percentage of tests generated by Pex
          = difficulties with Pex:
          == 2010/07/14: "tests generated between runs of Pex aren't always the same due to timeouts"
          == not aware of test double opportunities (i.e. mocks)
          == 2010/09/11: "time - it will give up/timeout to not be stuck"
          == 2010/09/11: "intent - unit testing is (validation +) verification exercise using specification/requirements, neither of which are available"
          === "Parameter of abstract type Stream (System.IO) is somewhat vague; there are many implementations of it that could be used, but for unit tests, a MemoryStream should probably ue used. Pex has no such guidance and seems to [randomly?] pick System.Security.Cryptography.TailStream which I can't find in System.Security nor in MSDN documentation!"
          == 2010/09/11: "no focus - unless code is decorated with exclude/ignore attributes"
          == 2010/09/11: "scope"
          === "testability issue" with certain methods, such as:
          ==== Environment.get_NewLine()
          ==== Environment.get_OSVersion()
          ==== File.*
          ==== FileStream..ctor
          === "uninstrumentable" with memory allocation & garbage collection
          === "uninstrumented" source code not available for analysis
          === "extern" behaviour only available at runtime; special case of "uninstrumented"
          === HttpWebRequest (System.Net) does not have a public constructor; the Remarks of the class say to call WebRequest.Create() (which isn't instrumented)
          === Neither does Match (System.Text.RegularExpressions) and it is not instrumented anyway.
          = evaluating with metrics (cyclomatic complexity)
          -->
          ...
        </command>
        <command name="section" param="Validation">
          <!-- Demonstrate that you met your goal
          Objectives can be checked off one by one
          If goal was "improve X", then show that X became better (i.e. less clicks, etc.)
          Verification isn't very useful; this really has to be about validation -->
          <!-- Cross-reference with Scope and discuss observed trade-offs of other potential approaches -->
          ...
        </command>
        <command name="section" param="Summary">
          ...
        </command>
      </command>
      <command name="chapter" param="Summary of work, Conclusions and Future Work">
        <command name="section" param="Introduction">
          <!-- what the chapter is about -->
          ...
        </command>
        <command name="section" param="Summary of results">
          <!-- Review goal and contributions -->
          ...
        </command>
        <command name="section" param="Conclusions">
          <!-- list of inferences made -->
          ...
        </command>
        <command name="section" param="Future work">
          <!-- best ideas of next steps -->
          ...
        </command>
      </command>
    </command>
    <command name="appendix">
      <command name="chapter" param="TODO">
        <!-- material of length that would impede the flow of the -->
        <!-- main body: program listings, long data results, long mathematical -->
        <!-- proofs -->
        <!-- TODO:
        = glossary, including abbreviations/TLAs
        -->
        ...
      </command>
    </command>
    <command name="bibliographystyle" param="abbrv" />
    <command name="bibliography" param="thesis" />
  </document>
</iTeX>